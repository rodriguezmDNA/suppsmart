{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e885c32a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:15:41.752034Z",
     "start_time": "2021-09-05T22:15:38.650186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfTransformer from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator TruncatedSVD from version 0.22.1 when using version 0.24.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### General use\n",
    "import os #interact with the system\n",
    "import pickle #save/load\n",
    "import re # regular expressions\n",
    "import pandas as pd #data frames\n",
    "import numpy as np #math\n",
    "from scipy import sparse #sparse matrices\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### For lemmatization\n",
    "import spacy #Language model\n",
    "\"\"\"\n",
    "See: https://spacy.io/models/en\n",
    "English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, \n",
    "POS tags, dependency parse and named entities\n",
    "\"\"\"\n",
    "\n",
    "### For stop words\n",
    "from wordcloud import STOPWORDS\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "### To create the matrices and perform similarity searches for the query\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "### gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "### Dviz\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##### Define unwanted/stop words\n",
    "\n",
    "\"\"\"\n",
    "-PRON- is a POS (part of speech) tag. In this case any pronoun (my, she, they, etc) is \n",
    "generalized (through lemmatization) to a -PRON- tag.\n",
    "This and other tags could be removed using another approach, but for simplicity I'm removing them along with\n",
    "other stop words\n",
    "\"\"\"\n",
    "## \n",
    "stop_words = stopwords.words('english')\n",
    "extraStop=[\"mg\",\"erowid\", \"-PRON-\",\"june\",'içŸ¥'] \n",
    "stop_words.extend(STOPWORDS)\n",
    "stop_words.extend(extraStop)\n",
    "stop_words.extend(stopwords.words('french')) # Just in case\n",
    "stop_words=set(stop_words)\n",
    "\n",
    "# Some words are actually desired, removing them from the set will keep them in the data.\n",
    "stop_words.discard(\"no\")\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#nlp = spacy.load('en', disable=['parser', 'ner']) \n",
    "# Functions for query search\n",
    "def cleanQuery(query):\n",
    "    \"\"\"\n",
    "    A function to clean the input text\n",
    "    \"\"\"\n",
    "    cleanQuery = \" \".join([_.lemma_ for _ in nlp(\" \".join(simple_preprocess(query)))])\n",
    "    cleanQuery = removeStopWords(cleanQuery,stop_words)\n",
    "    return cleanQuery\n",
    "\n",
    "def d2v_inferVector(searchTerms):\n",
    "    activationVector = doc2vec_model.infer_vector(doc_words=searchTerms.split(\" \"), epochs=200)\n",
    "    activationVector = activationVector.reshape(1, -1)\n",
    "    return activationVector #Activated neurons in NN in the doc2vec model\n",
    "\n",
    "def svd_inferVector(searchTerms):\n",
    "    \"\"\"\n",
    "    Find the embedding vector from the SVD decomposition\n",
    "    \"\"\"\n",
    "    bowSearch = tf.transform([searchTerms]).toarray() #no need to split here\n",
    "    bowSearch = svd.transform(bowSearch)\n",
    "    bowSearch = bowSearch.reshape(1, -1)\n",
    "    return bowSearch\n",
    "\n",
    "def removeStopWords(listOfText,stopWords):\n",
    "    return ' '.join([_ for _ in listOfText.split() if not _ in stopWords])\n",
    "\n",
    "\n",
    "def getSimilarDocument(matrix,vector):\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between the target matrix and the calculated vector\n",
    "    \"\"\"\n",
    "    cosSimilarity = pd.DataFrame(cosine_similarity(X=matrix,\n",
    "                                                    Y=vector,\n",
    "                                                    dense_output=True))\n",
    "    cosSimilarity.set_index(matrix.index, inplace=True)\n",
    "    cosSimilarity.columns = [\"cosine_similarity\"]\n",
    "    return cosSimilarity\n",
    "\n",
    "def aggregateCosSims(svd,d2v,topN=-1):\n",
    "    ### Average cosine similarity from doc2vec and also bag of words. \n",
    "    aggSimTable = pd.merge(d2v, svd, left_index=True, right_index=True)\n",
    "    aggSimTable.columns = [\"Doc2Vec\", \"BoW\"]\n",
    "    aggSimTable['aggCosSim'] = (aggSimTable[\"Doc2Vec\"] + aggSimTable[\"BoW\"])/2\n",
    "    aggSimTable.sort_values(by=\"aggCosSim\", ascending=False, inplace=True)\n",
    "    return aggSimTable.head(topN)\n",
    "\n",
    "def wrapperSimilarSupplements(query,topN=-1):\n",
    "    searchTerms = cleanQuery(query)\n",
    "    d2v_searchOut = d2v_inferVector(searchTerms)\n",
    "    svd_searchOut = svd_inferVector(searchTerms)\n",
    "    ##\n",
    "    svd_CosSim = getSimilarDocument(svd_feature_matrix,svd_searchOut)\n",
    "    d2v_CosSim = getSimilarDocument(doctovec_feature_matrix,d2v_searchOut)\n",
    "    topSimilarSupplements = aggregateCosSims(svd_CosSim,d2v_CosSim,topN)\n",
    "    namesOfSupps = getSupplementNames(topSimilarSupplements)\n",
    "    return namesOfSupps,topSimilarSupplements,searchTerms\n",
    "\n",
    "def loadModelData(dictOfSaves,pickleShelf = \"./modelData/\",verbose=False):\n",
    "    \"\"\"\n",
    "    Load results from model training\n",
    "    \"\"\"\n",
    "    tmpList = []\n",
    "    for fileName in dictOfSaves.values():\n",
    "        #\n",
    "        picklePath=pickleShelf + fileName\n",
    "        if verbose:\n",
    "            print(\"loading\",picklePath)       \n",
    "        with open(picklePath, 'rb') as handle:\n",
    "            tmpList.append(pickle.load(handle))\n",
    "    return tmpList\n",
    "\n",
    "###\n",
    "def make_bigrams(listOfTokens,bimodel):\n",
    "    return [bimodel[_] for _ in listOfTokens]\n",
    "\n",
    "def make_trigrams(listOfTokens,bimodel,trimodel):\n",
    "    return [trimodel[bimodel[_]] for _ in listOfTokens]\n",
    "\n",
    "def make_quadgrams(listOfTokens,bimodel,trimodel,quadmodel):\n",
    "    return [quadmodel[trimodel[bimodel[_]]] for _ in listOfTokens]\n",
    "\n",
    "def buildTopicModel(supplement,ngram=\"tri\",nTopics=8,modelType=\"lda\"):\n",
    "    tmpTokens = scrappedCleanTopic[supplement]\n",
    "    # ## For debugging\n",
    "    # tmpTokens = [(\"my cat is the best\").split(), \n",
    "    #              (\"my dog is the worst\").split(),\n",
    "    #              (\"my fish is the worst\").split(),\n",
    "    #              (\"i have one wish\").split(),\n",
    "    #             (\"i have one kind of shoe\").split(),\n",
    "    #             (\"i have one kind of dream\").split(),\n",
    "    #             (\"i have one kind of pillow\").split()]\n",
    "    # Build models\n",
    "    bigram = gensim.models.Phrases(tmpTokens, min_count=3, threshold=2) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[tmpTokens],min_count=3, threshold=2)  \n",
    "    quadgram = gensim.models.Phrases(trigram[bigram[tmpTokens]],min_count=3, threshold=2)  \n",
    "\n",
    "    # Get bi and trigrams\n",
    "    bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_model = gensim.models.phrases.Phraser(trigram)\n",
    "    quadgram_model = gensim.models.phrases.Phraser(quadgram)\n",
    "    \n",
    "    #print(quadgram_model[trigram_model[bigram_model[tmpTokens[4]]]])\n",
    "    tmp_bi = make_bigrams(tmpTokens,bigram_model)\n",
    "    tmp_tri = make_trigrams(tmpTokens,bigram_model,trigram_model)\n",
    "    tmp_quad = make_quadgrams(tmpTokens,bigram_model,trigram_model,quadgram_model)\n",
    "\n",
    "    ngramChoice = {\"bi\":tmp_bi, \n",
    "                   \"tri\":tmp_tri,\n",
    "                   \"quad\":tmp_quad}\n",
    "    \n",
    "    ngram = ngramChoice[ngram]\n",
    "    ###\n",
    "    id2word = gensim.corpora.Dictionary(ngram) # Dictionary\n",
    "    allWords = ngram #Corpus\n",
    "    corpus = [id2word.doc2bow(text) for text in allWords] #Freq of tokens\n",
    "    ##########\n",
    "    if(modelType == \"lda\"):\n",
    "        ldaMod = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                                   id2word=id2word,\n",
    "                                                   num_topics=nTopics, \n",
    "                                                   random_state=42,\n",
    "                                                   update_every=1,\n",
    "                                                   chunksize=100,\n",
    "                                                   passes=20,\n",
    "                                                   alpha='auto',\n",
    "                                                   per_word_topics=True)\n",
    "    else:\n",
    "        # Download File: http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
    "        mallet_path = 'mallet-2.0.8/bin/mallet' # update this path\n",
    "        ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=nTopics, id2word=id2word)\n",
    "        ldaMod = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "    return ldaMod,id2word,corpus\n",
    "\n",
    "\n",
    "def topicModelSupp(supp,nTopics=8,modelType='lda',ngram='bi'):\n",
    "    resLDA, resI2W, resCorp = buildTopicModel(supp,ngram,nTopics,modelType)\n",
    "    # Visualize the topics-keywords\n",
    "    #pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(resLDA, resCorp, resI2W)\n",
    "    return pyLDAvis.show(vis,port=8501) ## Working well with st.altair_chart\n",
    "    #return pyLDAvis.display(vis) #no pyplot,\n",
    "    #return pyLDAvis.prepared_data_to_html(vis) ## no st.write or mdown\n",
    "\n",
    "\n",
    "def hmTopical(topicTuple,topNumber):\n",
    "    resTopics = dict([list(reversed(_.split(\"*\"))) for _ in topicTuple.split('+')])\n",
    "    resTopics = {k:float(v) for k,v in resTopics.items()}\n",
    "    resTopicsDF = pd.DataFrame.from_dict(resTopics,orient='index',columns=[topNumber]) #.reset_index\n",
    "    return resTopicsDF\n",
    "\n",
    "prepDF = lambda resTopics: pd.concat([hmTopical(v,k) for k,v in resTopics.items()],axis=1).T\n",
    "\n",
    "def makeHM(supp,resTopicsDF):\n",
    "    plttitle = \"most representative words for \" + supp + \"\\n(\"+str(resTopicsDF.shape[0]) + \" topics)\"\n",
    "    #fig= plt.figure(figsize=(40,12))\n",
    "    return sns.heatmap(resTopicsDF,cmap=\"Blues\",square=True,cbar_kws={\"shrink\": .5}).set_title(plttitle)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "getSupplementNames = lambda df: list(df.index)\n",
    "\n",
    "\n",
    "loadPickles = {'doc2vec_model': 'doc2vec_model.pkl',\n",
    "               'svd_feature_matrix': 'svd_feature_matrix.pkl',\n",
    "               'doctovec_feature_matrix': 'doctovec_feature_matrix.pkl',\n",
    "               'tfidf_matrix': 'tfidf_matrix.pkl',\n",
    "               'tf': 'tf.pkl',\n",
    "               'svd': 'svd.pkl',\n",
    "               'svdMatrix': 'svdMatrix.pkl',\n",
    "               'scrappedCleanTopic':\"scrappedCleanTopic.pkl\"\n",
    "               }\n",
    "\n",
    "tmpList = loadModelData(loadPickles,verbose=False)\n",
    "doc2vec_model,svd_feature_matrix,doctovec_feature_matrix, tfidf_matrix, tf, svd, svdMatrix, scrappedCleanTopic = tmpList\n",
    "del tmpList #Stop holding into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5321c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:16:09.785349Z",
     "start_time": "2021-09-05T22:16:09.782565Z"
    }
   },
   "outputs": [],
   "source": [
    "query = 'I want something to help me sleep'\n",
    "topN = int(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ccd61d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c56eaf0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:19:23.097839Z",
     "start_time": "2021-09-05T22:19:23.092875Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x7fa40a55f040>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_model.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89bc7d61",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:17:32.344321Z",
     "start_time": "2021-09-05T22:17:32.327950Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'dv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/1487960111.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdoc2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudorandom_weak_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
     ]
    }
   ],
   "source": [
    "doc2vec_model.infer_vector(doc_words=searchTerms.split(\" \"), epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02c2f599",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:17:00.581215Z",
     "start_time": "2021-09-05T22:17:00.577474Z"
    }
   },
   "outputs": [],
   "source": [
    "def d2v_inferVector(searchTerms):\n",
    "    activationVector = doc2vec_model.infer_vector(doc_words=searchTerms.split(\" \"), epochs=200)\n",
    "    activationVector = activationVector.reshape(1, -1)\n",
    "    return activationVector #Activated neurons in NN in the doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90c2c3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:17:04.478227Z",
     "start_time": "2021-09-05T22:17:04.456070Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'dv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/624908882.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0md2v_inferVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/1091370396.py\u001b[0m in \u001b[0;36md2v_inferVector\u001b[0;34m(searchTerms)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0md2v_inferVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mactivationVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mactivationVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivationVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivationVector\u001b[0m \u001b[0;31m#Activated neurons in NN in the doc2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudorandom_weak_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
     ]
    }
   ],
   "source": [
    "d2v_inferVector(searchTerms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb076a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "svd_searchOut = svd_inferVector(searchTerms)\n",
    "##\n",
    "svd_CosSim = getSimilarDocument(svd_feature_matrix,svd_searchOut)\n",
    "d2v_CosSim = getSimilarDocument(doctovec_feature_matrix,d2v_searchOut)\n",
    "topSimilarSupplements = aggregateCosSims(svd_CosSim,d2v_CosSim,topN)\n",
    "namesOfSupps = getSupplementNames(topSimilarSupplements)\n",
    "return namesOfSupps,topSimilarSupplements,searchTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98779a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9abe22a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fd40981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-05T22:16:14.979462Z",
     "start_time": "2021-09-05T22:16:14.945137Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Doc2Vec' object has no attribute 'dv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/258151117.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrecList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecTable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msearchQuery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapperSimilarSupplements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/3130763157.py\u001b[0m in \u001b[0;36mwrapperSimilarSupplements\u001b[0;34m(query, topN)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwrapperSimilarSupplements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0msearchTerms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0md2v_searchOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md2v_inferVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0msvd_searchOut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd_inferVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/4m/7mpb_q5j65zdcnwbryv2_f8h0000gn/T/ipykernel_3652/3130763157.py\u001b[0m in \u001b[0;36md2v_inferVector\u001b[0;34m(searchTerms)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0md2v_inferVector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mactivationVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msearchTerms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0mactivationVector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivationVector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mactivationVector\u001b[0m \u001b[0;31m#Activated neurons in NN in the doc2vec model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda3/envs/suppsmart/lib/python3.8/site-packages/gensim/models/doc2vec.py\u001b[0m in \u001b[0;36minfer_vector\u001b[0;34m(self, doc_words, alpha, min_alpha, epochs)\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpseudorandom_weak_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0mdoctag_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoctag_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Doc2Vec' object has no attribute 'dv'"
     ]
    }
   ],
   "source": [
    "recList, recTable,searchQuery = wrapperSimilarSupplements(query,topN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f200569",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
